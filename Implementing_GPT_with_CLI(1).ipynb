{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "23a8b79e",
      "metadata": {
        "id": "23a8b79e"
      },
      "source": [
        "\n",
        "## GPT CLI Interface Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8be505",
      "metadata": {
        "id": "4f8be505"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a4d737",
      "metadata": {
        "id": "a1a4d737"
      },
      "outputs": [],
      "source": [
        "\n",
        "seed                   = 42\n",
        "max_length             = 150\n",
        "num_return_sequences   = 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function takes questions, and asks to the model generate multiple answers, stores them neatly in dictionaries, prints them, and finally returns them all.\n",
        "\n"
      ],
      "metadata": {
        "id": "9BVXk50HGsei"
      },
      "id": "9BVXk50HGsei"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60be09e",
      "metadata": {
        "id": "b60be09e"
      },
      "outputs": [],
      "source": [
        "def generate_examples( generator, prompt_list ):\n",
        "\n",
        "    set_seed(seed)\n",
        "    examples = []\n",
        "\n",
        "    for prompt in prompt_list:\n",
        "\n",
        "        result = generator(\n",
        "                   prompt,\n",
        "                   max_length           = max_length,\n",
        "                   num_return_sequences = num_return_sequences\n",
        "        )\n",
        "\n",
        "        example = {'prompt': prompt}\n",
        "\n",
        "        for i, res in enumerate( result ):\n",
        "            answer    = res['generated_text'].lstrip().strip()\n",
        "            example[f'answer{ i + 1 }'] = answer\n",
        "        examples.append(example)\n",
        "        print( json.dumps( example, indent = 2) )\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "478a6d46",
      "metadata": {
        "id": "478a6d46"
      },
      "source": [
        "\n",
        "## Get GPT2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### storing the name of the model"
      ],
      "metadata": {
        "id": "LJhb1aeAFVlz"
      },
      "id": "LJhb1aeAFVlz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f671e8e6",
      "metadata": {
        "id": "f671e8e6"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_name           = 'gpt2'\n",
        "\n",
        "model_gpt_generator  = pipeline('text-generation', model=model_name )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here, A Hugging Face class that automatically picks the right tokenizer for a model"
      ],
      "metadata": {
        "id": "Vur3I7RQCbJP"
      },
      "id": "Vur3I7RQCbJP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd90ec3c",
      "metadata": {
        "id": "bd90ec3c"
      },
      "outputs": [],
      "source": [
        "tokenizer              = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  In, This line in of code i'll passes the input propt sentence to GPT Modle through a custom function"
      ],
      "metadata": {
        "id": "Udr5xNDoGCeI"
      },
      "id": "Udr5xNDoGCeI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c9a2df",
      "metadata": {
        "id": "22c9a2df"
      },
      "outputs": [],
      "source": [
        "\n",
        "list_to_answer = [\"it will be foggy weather today?\"]\n",
        "\n",
        "say_something = generate_examples( model_gpt_generator, list_to_answer )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07cc0f9f",
      "metadata": {
        "id": "07cc0f9f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5105ee66",
      "metadata": {
        "id": "5105ee66"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}